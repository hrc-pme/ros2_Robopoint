#!/usr/bin/env python3
"""
ROS2 Model Worker Node - åŸ·è¡Œå¤šæ¨¡æ…‹æ¨¡å‹çš„ ROS2 ç¯€é»
ä¿®å¾©ç‰ˆæœ¬ï¼šç§»é™¤èˆ‡ Controller è¡çªçš„æœå‹™ï¼Œå°ˆæ³¨æ–¼æ¨¡å‹æœå‹™
"""
import json
import time
import threading
import uuid
import base64
import io
from typing import Optional, List, Dict, Any

import torch
import rclpy
from rclpy.node import Node
from rclpy.callback_groups import ReentrantCallbackGroup
from rclpy.executors import MultiThreadedExecutor
from std_msgs.msg import String
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
from transformers import TextIteratorStreamer
from threading import Thread, Semaphore
from PIL import Image as PILImage

# è‡ªå®šç¾©è¨Šæ¯é¡å‹
from robopoint_interfaces.msg import GenerateRequest, GenerateResponse, WorkerStatus
from robopoint_interfaces.srv import WorkerGetStatus, WorkerGenerateStream, RegisterAllWorkers

from robopoint_worker.model.builder import load_pretrained_model
from robopoint_worker.mm_utils import process_images, load_image_from_base64, tokenizer_image_token
from robopoint_controller.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN
from robopoint_controller.utils import build_logger, server_error_msg


class ROS2ModelWorker(Node):
    def __init__(self):
        super().__init__('model_worker')
        
        # è²æ˜åƒæ•¸
        self.declare_parameters(
            namespace='',
            parameters=[
                ('model_path', 'facebook/opt-350m'),
                ('model_base', ''),
                ('model_name', ''),
                ('device', 'cuda'),
                ('load_8bit', False),
                ('load_4bit', False),
                ('use_flash_attn', False),
                ('limit_model_concurrency', 5),
                ('heartbeat_interval', 10.0),
                ('controller_address', '/model_controller'),
                ('worker_id', str(uuid.uuid4())[:6])
            ]
        )
        
        # ç²å–åƒæ•¸
        self.model_path = self.get_parameter('model_path').value
        self.model_base = self.get_parameter('model_base').value
        self.model_name = self.get_parameter('model_name').value
        self.device = self.get_parameter('device').value
        self.load_8bit = self.get_parameter('load_8bit').value
        self.load_4bit = self.get_parameter('load_4bit').value
        self.use_flash_attn = self.get_parameter('use_flash_attn').value
        self.limit_model_concurrency = self.get_parameter('limit_model_concurrency').value
        self.heartbeat_interval = self.get_parameter('heartbeat_interval').value
        self.controller_address = self.get_parameter('controller_address').value
        self.worker_id = self.get_parameter('worker_id').value
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.callback_group = ReentrantCallbackGroup()
        self.cv_bridge = CvBridge()
        self.logger = build_logger("ros2_model_worker", f"ros2_model_worker_{self.worker_id}.log")
        self.global_counter = 0
        
        # æ¨¡å‹ä¸¦ç™¼æ§åˆ¶
        self.model_semaphore = Semaphore(self.limit_model_concurrency)
        
        # æ¨¡å‹åŠ è¼‰ç‹€æ…‹
        self.model_loaded = False
        
        # è™•ç†æ¨¡å‹è·¯å¾‘å’Œåç¨±
        if self.model_path.endswith("/"):
            self.model_path = self.model_path[:-1]
            
        if not self.model_name:
            model_paths = self.model_path.split("/")
            if model_paths[-1].startswith('checkpoint-'):
                self.model_name = model_paths[-2] + "_" + model_paths[-1]
            else:
                self.model_name = model_paths[-1]
        
        # è¼‰å…¥æ¨¡å‹
        self.get_logger().info(f'Loading model {self.model_name} on worker {self.worker_id}...')
        try:
            self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(
                self.model_path, 
                self.model_base if self.model_base else None, 
                self.model_name, 
                self.load_8bit, 
                self.load_4bit, 
                device=self.device, 
                use_flash_attn=self.use_flash_attn
            )
            self.model_loaded = True
            self.get_logger().info(f'Model {self.model_name} loaded successfully')
        except Exception as e:
            self.get_logger().error(f'Failed to load model: {e}')
            self.tokenizer = None
            self.model = None
            self.image_processor = None
            self.context_len = 2048
            self.model_loaded = False
        
        # ğŸ”§ å»ºç«‹ publishers - å¿ƒè·³ç™¼é€åˆ° Controller
        self.heartbeat_pub = self.create_publisher(
            WorkerStatus, 
            '/worker_heartbeat',  # Controller çµ±ä¸€æ¥æ”¶å¿ƒè·³çš„ topic
            10
        )
        
        self.response_pub = self.create_publisher(
            GenerateResponse,
            f'/model_worker_{self.worker_id}/response',
            10
        )
        
        # å»ºç«‹ subscribers
        self.request_sub = self.create_subscription(
            GenerateRequest,
            f'/model_worker_{self.worker_id}/request',
            self.generate_callback,
            10,
            callback_group=self.callback_group
        )
        
        # ğŸ”§ å»ºç«‹ services - åªä¿ç•™ Worker ç‰¹æœ‰çš„æœå‹™
        self.status_service = self.create_service(
            WorkerGetStatus,
            f'/model_worker_{self.worker_id}/worker_get_status',  # ä½¿ç”¨ Worker ç‰¹å®šè·¯å¾‘
            self.get_status_callback,
            callback_group=self.callback_group
        )
        
        # ğŸ”§ é—œéµï¼šåªæä¾› WorkerGenerateStream æœå‹™ï¼Œç§»é™¤èˆ‡ Controller è¡çªçš„æœå‹™
        self.worker_generate_service = self.create_service(
            WorkerGenerateStream,
            f'/model_worker_{self.worker_id}/generate_stream',
            self.handle_worker_generate_stream,
            callback_group=self.callback_group
        )
        
        # ğŸ”§ æ–°å¢ï¼šå‘ Controller è¨»å†Šç”¨çš„ service clients
        self.register_client = self.create_client(
            RegisterAllWorkers,
            '/register_all_workers'
        )
        
        # å»ºç«‹ heartbeat timer
        self.heartbeat_timer = self.create_timer(
            self.heartbeat_interval,
            self.send_heartbeat,
            callback_group=self.callback_group
        )
        
        # è¨»å†Šåˆ°æ§åˆ¶å™¨
        self.register_to_controller()
        
        self.get_logger().info(f'Model worker {self.worker_id} initialized successfully')
        self.get_logger().info(f'Available services:')
        self.get_logger().info(f'  - /model_worker_{self.worker_id}/generate_stream')
        self.get_logger().info(f'  - /model_worker_{self.worker_id}/worker_get_status')
    
    def register_to_controller(self):
        """è¨»å†Šåˆ°æ§åˆ¶å™¨"""
        self.get_logger().info('Registering to controller...')
        
        # ç­‰å¾… Controller çš„è¨»å†Šæœå‹™å¯ç”¨
        if self.register_client.wait_for_service(timeout_sec=10.0):
            # é€šé service call è¨»å†Š
            register_request = RegisterAllWorkers.Request()
            register_request.worker_name = self.worker_id
            register_request.check_heart_beat = True
            register_request.worker_status = json.dumps({
                "model_names": [self.model_name],
                "speed": 1.0,
                "queue_length": self.get_queue_length()
            })
            
            future = self.register_client.call_async(register_request)
            rclpy.spin_until_future_complete(self, future, timeout_sec=5.0)
            
            if future.result() and future.result().success:
                self.get_logger().info(f'Successfully registered to controller')
            else:
                self.get_logger().warn(f'Failed to register to controller')
        else:
            self.get_logger().warn('Controller registration service not available')
        
        # åŒæ™‚é€šé topic ç™¼å¸ƒè¨»å†Šè¨Šæ¯ï¼ˆä½œç‚ºå‚™ç”¨ï¼‰
        status_msg = WorkerStatus()
        status_msg.worker_id = self.worker_id
        status_msg.model_names = self.model_name
        status_msg.queue_length = self.get_queue_length()
        status_msg.is_registration = True
        
        self.heartbeat_pub.publish(status_msg)
    
    def send_heartbeat(self):
        """ç™¼é€å¿ƒè·³è¨Šæ¯åˆ° Controller"""
        status_msg = WorkerStatus()
        status_msg.worker_id = self.worker_id
        status_msg.model_names = self.model_name
        status_msg.queue_length = self.get_queue_length()
        status_msg.global_counter = self.global_counter
        status_msg.is_registration = False
        
        self.heartbeat_pub.publish(status_msg)
        
        self.get_logger().debug(
            f'Heartbeat sent - Models: {[self.model_name]}, '
            f'Queue: {self.get_queue_length()}, '
            f'Counter: {self.global_counter}'
        )
    
    def get_queue_length(self) -> int:
        """ç²å–ä½‡åˆ—é•·åº¦"""
        if self.model_semaphore is None:
            return 0
        return self.limit_model_concurrency - self.model_semaphore._value
    
    def get_status_callback(self, request, response):
        """è™•ç†ç‹€æ…‹æŸ¥è©¢æœå‹™"""
        response.worker_id = self.worker_id
        response.model_names = json.dumps([self.model_name])  # è¿”å› JSON å­—ç¬¦ä¸²
        response.speed = 1
        response.queue_length = self.get_queue_length()
        return response
    
    def handle_worker_generate_stream(self, request, response):
        """è™•ç† WorkerGenerateStream è«‹æ±‚ - é€™æ˜¯ RoboPointNode éœ€è¦èª¿ç”¨çš„æœå‹™"""
        self.get_logger().info(f"Received WorkerGenerateStream request")
        self.get_logger().info(f"  Model: {request.model}")
        self.get_logger().info(f"  Prompt length: {len(request.prompt)}")
        self.get_logger().info(f"  Images count: {len(request.images)}")
        
        try:
            if not self.model_loaded:
                # å¦‚æœæ¨¡å‹æœªè¼‰å…¥ï¼Œè¿”å›æ¨¡æ“¬çµæœ
                response.success = True
                response.text = request.prompt + " [(0.5, 0.3), (0.7, 0.6)]"
                response.error_message = ""
                self.get_logger().info("Returning simulated response (model not loaded)")
                return response
            
            # å¯¦éš›ç”Ÿæˆ
            generated_text = self._generate_with_model(request)
            
            response.success = True
            response.text = generated_text
            response.error_message = ""
            
            self.get_logger().info(f"Generation completed, response length: {len(response.text)}")
            
        except Exception as e:
            self.get_logger().error(f'WorkerGenerateStream error: {e}')
            response.success = False
            response.text = ""
            response.error_message = str(e)
        
        return response
    
    def _generate_with_model(self, request):
        """å¯¦éš›çš„æ¨¡å‹ç”Ÿæˆé‚è¼¯"""
        try:
            # è™•ç†åœ–åƒ
            images = None
            if request.images:
                self.get_logger().info(f"Processing {len(request.images)} images")
                # å°‡ base64 åœ–åƒè½‰æ›ç‚º PIL åœ–åƒ
                pil_images = []
                for img_base64 in request.images:
                    img_data = base64.b64decode(img_base64)
                    pil_image = PILImage.open(io.BytesIO(img_data))
                    pil_images.append(pil_image)
                    self.get_logger().info(f"  Image size: {pil_image.size}")
                
                # è™•ç†åœ–åƒ
                if self.image_processor:
                    images = process_images(pil_images, self.image_processor, self.model.config)
                    if isinstance(images, list):
                        images = [img.to(self.device, dtype=torch.float16) for img in images]
                    else:
                        images = images.to(self.device, dtype=torch.float16)
            
            # è™•ç†æç¤ºè©
            prompt = request.prompt
            original_prompt = prompt
            
            if images is not None and hasattr(self, 'tokenizer'):
                # æ›¿æ›åœ–åƒæ¨™è¨˜
                replace_token = DEFAULT_IMAGE_TOKEN
                if hasattr(self.model.config, 'mm_use_im_start_end') and self.model.config.mm_use_im_start_end:
                    replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN
                prompt = prompt.replace(DEFAULT_IMAGE_TOKEN, replace_token)
                self.get_logger().info("Image tokens processed")
            
            # TokenåŒ–ï¼ˆå¦‚æœæœ‰ tokenizerï¼‰
            if hasattr(self, 'tokenizer') and self.tokenizer:
                input_ids = tokenizer_image_token(
                    prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt'
                ).unsqueeze(0).to(self.device)
                
                # ç”Ÿæˆåƒæ•¸
                temperature = max(float(request.temperature), 0.001) if request.temperature > 0 else 1.0
                top_p = float(request.top_p) if request.top_p > 0 else 1.0
                max_new_tokens = min(int(request.max_new_tokens), 512) if request.max_new_tokens > 0 else 256
                
                self.get_logger().info(f"Generation params: temp={temperature}, top_p={top_p}, max_tokens={max_new_tokens}")
                
                # ç”Ÿæˆ
                with torch.inference_mode():
                    if images is not None:
                        output_ids = self.model.generate(
                            input_ids,
                            images=images,
                            do_sample=temperature > 0.001,
                            temperature=temperature,
                            top_p=top_p,
                            max_new_tokens=max_new_tokens,
                            use_cache=True,
                        )
                    else:
                        output_ids = self.model.generate(
                            input_ids,
                            do_sample=temperature > 0.001,
                            temperature=temperature,
                            top_p=top_p,
                            max_new_tokens=max_new_tokens,
                            use_cache=True,
                        )
                
                # è§£ç¢¼è¼¸å‡º
                input_token_len = input_ids.shape[1]
                generated_ids = output_ids[0][input_token_len:]
                generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
                
                return original_prompt + generated_text
            else:
                # æ²’æœ‰ tokenizerï¼Œè¿”å›æ¨¡æ“¬çµæœ
                return request.prompt + " [(0.5, 0.3), (0.7, 0.6)]"
                
        except Exception as e:
            self.get_logger().error(f"Error in model generation: {e}")
            # è¿”å›æ¨¡æ“¬çµæœä½œç‚ºå¾Œå‚™
            return request.prompt + " [(0.5, 0.3), (0.7, 0.6)]"
    
    def generate_callback(self, msg: GenerateRequest):
        """è™•ç†ç”Ÿæˆè«‹æ±‚"""
        self.global_counter += 1
        
        # ç²å–ä¿¡è™Ÿé‡
        self.model_semaphore.acquire()
        
        try:
            # åœ¨å¾Œå°åŸ·è¡Œç”Ÿæˆ
            thread = Thread(
                target=self._generate_stream_thread,
                args=(msg,),
                daemon=True
            )
            thread.start()
            
        except Exception as e:
            self.get_logger().error(f'Error starting generation thread: {e}')
            self.model_semaphore.release()
    
    def _generate_stream_thread(self, request: GenerateRequest):
        """åœ¨ç¨ç«‹åŸ·è¡Œç·’ä¸­åŸ·è¡Œç”Ÿæˆ"""
        try:
            for response in self._generate_stream(request):
                self.response_pub.publish(response)
        except Exception as e:
            self.get_logger().error(f'Generation error: {e}')
            # ç™¼é€éŒ¯èª¤å›æ‡‰
            error_response = GenerateResponse()
            error_response.request_id = request.request_id
            error_response.text = server_error_msg
            error_response.error_code = 1
            error_response.is_final = True
            self.response_pub.publish(error_response)
        finally:
            self.model_semaphore.release()
    
    @torch.inference_mode()
    def _generate_stream(self, request: GenerateRequest):
        """ç”Ÿæˆå›æ‡‰æµ"""
        if not self.model_loaded:
            # å¦‚æœæ¨¡å‹æœªè¼‰å…¥ï¼Œè¿”å›æ¨¡æ“¬çµæœ
            response = GenerateResponse()
            response.request_id = request.request_id
            response.text = request.prompt + " [(0.5, 0.3), (0.7, 0.6)]"
            response.error_code = 0
            response.is_final = True
            yield response
            return
            
        tokenizer, model, image_processor = self.tokenizer, self.model, self.image_processor
        
        prompt = request.prompt
        ori_prompt = prompt
        images = None
        num_image_tokens = 0
        
        # è™•ç†åœ–åƒ
        if request.images:
            if len(request.images) != prompt.count(DEFAULT_IMAGE_TOKEN):
                raise ValueError("Number of images does not match number of <image> tokens in prompt")
            
            # å¾ ROS Image è¨Šæ¯è½‰æ›ç‚º PIL åœ–åƒ
            pil_images = []
            for img_msg in request.images:
                cv_image = self.cv_bridge.imgmsg_to_cv2(img_msg, "rgb8")
                pil_image = PILImage.fromarray(cv_image)
                pil_images.append(pil_image)
            
            image_sizes = [img.size for img in pil_images]
            images = process_images(pil_images, image_processor, model.config)
            
            if isinstance(images, list):
                images = [img.to(self.device, dtype=torch.float16) for img in images]
            else:
                images = images.to(self.device, dtype=torch.float16)
            
            replace_token = DEFAULT_IMAGE_TOKEN
            if getattr(model.config, 'mm_use_im_start_end', False):
                replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN
            prompt = prompt.replace(DEFAULT_IMAGE_TOKEN, replace_token)
            
            num_image_tokens = prompt.count(replace_token) * model.get_vision_tower().num_patches
            image_args = {"images": images, "image_sizes": image_sizes}
        else:
            image_args = {}
        
        # ç”Ÿæˆåƒæ•¸
        temperature = max(float(request.temperature), 0.001) if request.temperature > 0 else 1.0
        top_p = float(request.top_p) if request.top_p > 0 else 1.0
        max_new_tokens = min(int(request.max_new_tokens), 1024) if request.max_new_tokens > 0 else 256
        stop_str = request.stop if request.stop else None
        do_sample = temperature > 0.001
        
        # Token åŒ–
        input_ids = tokenizer_image_token(
            prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt'
        ).unsqueeze(0).to(self.device)
        
        # è¨­å®šä¸²æµå™¨
        streamer = TextIteratorStreamer(
            tokenizer, skip_prompt=True, skip_special_tokens=True, timeout=15
        )
        
        max_context_length = getattr(model.config, 'max_position_embeddings', 2048)
        max_new_tokens = min(max_new_tokens, max_context_length - input_ids.shape[-1] - num_image_tokens)
        
        if max_new_tokens < 1:
            response = GenerateResponse()
            response.request_id = request.request_id
            response.text = ori_prompt + "Exceeds max token length. Please start a new conversation, thanks."
            response.error_code = 0
            response.is_final = True
            yield response
            return
        
        # å•Ÿå‹•ç”ŸæˆåŸ·è¡Œç·’
        generation_thread = Thread(
            target=model.generate,
            kwargs=dict(
                inputs=input_ids,
                do_sample=do_sample,
                temperature=temperature,
                top_p=top_p,
                max_new_tokens=max_new_tokens,
                streamer=streamer,
                use_cache=True,
                **image_args
            )
        )
        generation_thread.start()
        
        # ä¸²æµç”Ÿæˆçµæœ
        generated_text = ori_prompt
        for new_text in streamer:
            generated_text += new_text
            if stop_str and generated_text.endswith(stop_str):
                generated_text = generated_text[:-len(stop_str)]
            
            response = GenerateResponse()
            response.request_id = request.request_id
            response.text = generated_text
            response.error_code = 0
            response.is_final = False
            yield response
        
        # ç™¼é€æœ€çµ‚å›æ‡‰
        final_response = GenerateResponse()
        final_response.request_id = request.request_id
        final_response.text = generated_text
        final_response.error_code = 0
        final_response.is_final = True
        yield final_response


def main(args=None):
    rclpy.init(args=args)
    
    worker = ROS2ModelWorker()
    
    # ä½¿ç”¨å¤šåŸ·è¡Œç·’åŸ·è¡Œå™¨
    executor = MultiThreadedExecutor()
    executor.add_node(worker)
    
    try:
        executor.spin()
    except KeyboardInterrupt:
        pass
    finally:
        worker.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()